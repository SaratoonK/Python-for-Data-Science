{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch for Linear Regression\n",
    "\n",
    "So we can understand how PyTorch works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#as you all know, things can speed up if you have NVIDIA GPU\n",
    "#CUDA is the framework that NVIDIA develops, which allows us to use the GPU for calculations\n",
    "\n",
    "#my PC is MAC, and I don't have CUDA\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan for today:\n",
    "\n",
    "1. ETL \n",
    "   1. Specifying some some random input\n",
    "   2. PyTorch Dataset and DataLoader\n",
    "2. EDA - we gonna just skip because we are lazy...\n",
    "3. Feature Engineering / Cleaning - which we don't need to....\n",
    "4. Modeling \n",
    "   1. `nn.Linear` (luckily, you already understand this!  Yay!)\n",
    "   2. Define loss function (mse for regression, ce for classification)\n",
    "   3. Define the optimizer function (gradient descent ; adam)\n",
    "   4. Train the model\n",
    "5. Inference / Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ETL\n",
    "\n",
    "### 1.1 Specify some input\n",
    "\n",
    "Consider this data:\n",
    "\n",
    "<img src = \"../figures/japan.png\" width=\"500\">\n",
    "\n",
    "In a linear regression model, each target variable is estimated to be a weighted sum of the input variables, offset by some constant, known as a bias :\n",
    "\n",
    "$$\\text{yield}_\\text{apple}  = w_{11} * \\text{temp} + w_{12} * \\text{rainfall} + w_{13} * \\text{humidity} + b_{1}$$\n",
    "\n",
    "$$\\text{yield}_\\text{orange} = w_{21} * \\text{temp} + w_{22} * \\text{rainfall} + w_{23} * \\text{humidity} + b_{2}$$\n",
    "\n",
    "Visually, it means that the yield of apples is a linear or planar function of temperature, rainfall and humidity:\n",
    "\n",
    "<img src = \"../figures/japan2.png\" width=\"400\">\n",
    "\n",
    "The learning part of linear regression is to figure out a set of weights <code>w11, w12,... w23, b1 \\& b2</code> using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X (temp, rainfall, hum)\n",
    "X_train = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], \n",
    "                   [102, 43, 37], [69, 96, 70], [73, 67, 43], \n",
    "                   [91, 88, 64], [87, 134, 58], [102, 43, 37], \n",
    "                   [69, 96, 70], [73, 67, 43], [91, 88, 64], \n",
    "                   [87, 134, 58], [102, 43, 37], [69, 96, 70]], \n",
    "                  dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "Y_train = np.array([[56, 70], [81, 101], [119, 133], \n",
    "                    [22, 37], [103, 119], [56, 70], \n",
    "                    [81, 101], [119, 133], [22, 37], \n",
    "                    [103, 119], [56, 70], [81, 101], \n",
    "                    [119, 133], [22, 37], [103, 119]], \n",
    "                   dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([15, 3]), torch.Size([15, 2]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#please create tensors from these numpy array\n",
    "#torch.from_numpy (copy)  or torch.tensor  (not a copy!)\n",
    "inputs  = torch.tensor(X_train)\n",
    "targets = torch.tensor(Y_train)\n",
    "\n",
    "#please print the shape of these tensors\n",
    "#use either .size() or .shape\n",
    "inputs.shape, targets.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dataset\n",
    "\n",
    "We gonna create `TensorDataset` on top of these tensors, so we can access each row from inputs and targets as tuples.   \n",
    "\n",
    "Note:  This must be done, if we want to use `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put this dataset on top of our inputs and targets\n",
    "#format: TensorDataset(X, y) where X.shape is (m, n) and y.shape is (m, k)\n",
    "ds = TensorDataset(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([91., 88., 64.]), tensor([ 81., 101.]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[1] #this is a tuple of two tensors, the x and the corresponding y\n",
    "#this IS THE FORMAT that pyTorch wants!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 DataLoader\n",
    "\n",
    "By default, PyTorch works in batch (remember the mini-batch gradient descent!).\n",
    "\n",
    "In simple words, it will ALWAYS take some mini-batch, and perform gradient descent.\n",
    "\n",
    "Why PyTorch assume mini-batch; because PyTorch assumes you won't be able to fit in ~1M samples into your GPU ram....(3, 4, 6, 11, 12, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this dataloader will automatically create an enumerator, look at each batch\n",
    "#means, you can simply perform a for loop onto this dataloader\n",
    "#if you DON'T WANT TO use this DataLoader, it's fine!  But you have\n",
    "#to manually select the mini-batch (just like we do in our LR mini-batch class)\n",
    "from torch.utils.data import DataLoader  #this guy is randomized (if you set Shuffle=true)\n",
    "\n",
    "batch_size = 3  #this is any number you like\n",
    "#too small then your code runs slow\n",
    "#too big then you may get \"out of memory\" error\n",
    "\n",
    "dl = DataLoader(ds, batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, this dl is basically an enumerator, in which we can loop on....\n",
    "\n",
    "# for x, y in dl:\n",
    "#     print(f\"X: {x}\")  \n",
    "#     print(f\"Y: {y}\")\n",
    "#     break\n",
    "\n",
    "#this dl has an internal counter, that keeps where it is currently\n",
    "#this dl keeps on running; which is intentional; because we have the concept of \"epochs\"\n",
    "#epochs means that how many times we \"exhaust\" the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA - skip because we are lazy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define our neural network\n",
    "\n",
    "- how many linear layers we want???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn #stands for neural network; modules that contains many possible layers\n",
    "#define our neural network\n",
    "#just use one layer....\n",
    "#we gonna come back here and add one more layer....\n",
    "#format: nn.Linear(in_features, out_features)\n",
    "#format: nn.Linear(temp;rainfall;hum  ,  orange; apples)\n",
    "model = nn.Linear(3, 2)\n",
    "\n",
    "#linear layers are basically simple matrix multiplication....\n",
    "#Many other names:  In Keras, we called Dense.  In TensorFlow, we called FullyConnected\n",
    "\n",
    "#Keras are very high-level - not good for research / development (mainly for education...)\n",
    "#TensorFlow is developed by Google - it's quite good\n",
    "\n",
    "#for very huge, complex, high performance model - TensorFlow is much better / optimized\n",
    "    #they are more low-level than PyTorch\n",
    "#for very generally almost any model that we use (even in research) - PyTorch is much better \n",
    "# due to its computational graph.....\n",
    "\n",
    "#TensorFlow supports something called TensorFlowLite, which is the way\n",
    "#you want to use for mobile phones...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0085, -0.0723,  0.1650],\n",
       "        [-0.0224, -0.3495,  0.0397]], requires_grad=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight #by default, these weights are uniformly close to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight.shape  #this one is basically in the shape (out_features, in_feature)\n",
    "\n",
    "#you can imagine X @ W^T\n",
    "#after you transpose W, W^T becomes [3, 2]\n",
    "#which now you can do X @ W^T which is (anything, 3) @ (3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.2132,  0.2043], requires_grad=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bias  #why two bias???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0085, -0.0723,  0.1650],\n",
       "         [-0.0224, -0.3495,  0.0397]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.2132,  0.2043], requires_grad=True)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())  #this will list all the parameters (it's a object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p.numel() just flatten everything....\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "#why 8 here??? - 6 weights and 2 bias....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=3, out_features=2, bias=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so how do we use our model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 3])\n",
      "torch.Size([15, 2])\n"
     ]
    }
   ],
   "source": [
    "#so you can perform a forward pass, simply using \n",
    "#format: model(inputs)\n",
    "\n",
    "print(\"Inputs: \", inputs.shape)\n",
    "\n",
    "output = model(inputs)  #(15, 3) @ (3, 2) = (15, 2)\n",
    "print(output.shape)  #why output.shape is 15, 2??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Define the loss function\n",
    "\n",
    "- should we use MSE or Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#under the nn module, there are many loss function\n",
    "J_fn = nn.MSELoss()\n",
    "\n",
    "#later on, you will know how to use this....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Define the optimizer\n",
    "- Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normally, in sklearn, we simply call fit, and it will do gradient descent\n",
    "#in code from scratch, we need to like specify how we want to update the gradients\n",
    "#optimizer handles how we update the parameters\n",
    "#   if we use w = w - alpha (gradient) ==> gradient descent\n",
    "#optimizer is handles by the `torch.optim` module\n",
    "#stochastic gradient descent ==> is NOT one sample; is basically mini-batch \n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Actually train the model\n",
    "\n",
    "- 1. Predict\n",
    "- 2. Loss\n",
    "- 3. Gradient\n",
    "- 4. Update the weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('teaching_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "becc4c8e5ad229b2591d820334d85e3db0111492344629bf57f272470dce75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
