{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the rule of today game is the following:\n",
    "#if 8 person submit the answer in the chat, then we continue...\n",
    "#so if you guys are faster, than we can finish this early...\n",
    "#chaky will not CODE a single line....he will only copy you guys code...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/income.csv')\n",
    "#get this file from the google classroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>workclass</th>\n",
       "      <th>occupation</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>income</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>Male</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Private</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>40</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>Male</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Married</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>50</td>\n",
       "      <td>&gt;50K</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59</td>\n",
       "      <td>Male</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Self-emp</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>20</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38</td>\n",
       "      <td>Female</td>\n",
       "      <td>Prof-school</td>\n",
       "      <td>15</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Federal-gov</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>57</td>\n",
       "      <td>&gt;50K</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>Female</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Private</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>40</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex    education  education-num marital-status    workclass  \\\n",
       "0   27    Male      HS-grad              9  Never-married      Private   \n",
       "1   47    Male      Masters             14        Married    Local-gov   \n",
       "2   59    Male      HS-grad              9       Divorced     Self-emp   \n",
       "3   38  Female  Prof-school             15  Never-married  Federal-gov   \n",
       "4   64  Female         11th              7        Widowed      Private   \n",
       "\n",
       "        occupation  hours-per-week income  label  \n",
       "0     Craft-repair              40  <=50K      0  \n",
       "1  Exec-managerial              50   >50K      1  \n",
       "2   Prof-specialty              20  <=50K      0  \n",
       "3   Prof-specialty              57   >50K      1  \n",
       "4  Farming-fishing              40  <=50K      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.723333\n",
       "1    0.276667\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#please check the class label - test task\n",
    "df['label'].value_counts(normalize=True)\n",
    "#again, deep learning should be able to handle this for us\n",
    "#but you can always use imblearn library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Categorify\n",
    "\n",
    "Please categorify all categorical columns\n",
    "\n",
    "sex, education-num, marital-status, workclass, occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols  = ['sex', 'education-num', 'marital-status', 'workclass', 'occupation']\n",
    "cont_cols = ['age', 'hours-per-week'] \n",
    "y         = ['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype   \n",
      "---  ------          --------------  -----   \n",
      " 0   age             30000 non-null  int64   \n",
      " 1   sex             30000 non-null  category\n",
      " 2   education       30000 non-null  object  \n",
      " 3   education-num   30000 non-null  category\n",
      " 4   marital-status  30000 non-null  category\n",
      " 5   workclass       30000 non-null  category\n",
      " 6   occupation      30000 non-null  category\n",
      " 7   hours-per-week  30000 non-null  int64   \n",
      " 8   income          30000 non-null  object  \n",
      " 9   label           30000 non-null  int64   \n",
      "dtypes: category(5), int64(3), object(2)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "#change all cat_cols in type category\n",
    "#why need to categorify?\n",
    "#underhood:  things become integers, but you don't lose the string\n",
    "\n",
    "for col in cat_cols:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create embedding sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1), (14, 7), (6, 3), (5, 2), (12, 6)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_size = [len(df[col].cat.categories) for col in cat_cols]\n",
    "emb_size = [(size,min(50,size//2)) for size in cat_size]\n",
    "emb_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so can you guys explain to yourself, e.g., what is (12, 6) mean?\n",
    "# df['occupation'].unique()\n",
    "#each of the occupation is represent by a unique vector of size 6...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combine all categorical column into one tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30000, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats = np.stack([df[col].cat.codes.values for col in cat_cols], 1)\n",
    "cats = torch.tensor(cats, dtype=torch.int32)\n",
    "cats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combine all continuous column into one tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30000, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conts = np.stack([df[col].values for col in cont_cols], 1)\n",
    "conts = torch.tensor(conts, dtype=torch.float32)\n",
    "conts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Turn your y into tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#you should be able to answer what is shape of y that pyTorch likes\n",
    "#(sample) or (sample, 1) or (sample, target)  -->  (sample)\n",
    "y = torch.tensor(df[y].values).flatten()  #reshape(-1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create your training and testing set for cat and cont and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 6000\n",
    "test_size  = 1000\n",
    "#why Chaky don't use PyTorch datasets or dataloader?\n",
    "#silly question, why Chaky don't use all the dataset (30,000)\n",
    "#actually, Chaky should have use validation set\n",
    "cat_train = cats[:train_size]\n",
    "cat_test  = cats[train_size:test_size+train_size]\n",
    "con_train = conts[:train_size]\n",
    "con_test  = conts[train_size:test_size+train_size]\n",
    "y_train   = y[:train_size]\n",
    "y_test    = y[train_size:test_size+train_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the exact model as yesterday\n",
    "class IncomeModel(nn.Module):\n",
    "    def __init__(self, emb_size, cont_size, out_size, layer_size = [200,100], p = 0.5):\n",
    "        super().__init__()\n",
    "        self.embed_layer = nn.ModuleList([nn.Embedding(unique, emb_s) for unique, emb_s in emb_size])\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.batchnorm1d = nn.BatchNorm1d(cont_size)\n",
    "\n",
    "        cat_size = sum(emb_s for _, emb_s in emb_size)\n",
    "        input_size = cat_size + cont_size\n",
    "        layerlist = []\n",
    "        for i in layer_size:\n",
    "            layerlist.append(nn.Linear(input_size,i)) #(input_size,200)\n",
    "            layerlist.append(nn.ReLU(inplace=True)) \n",
    "            layerlist.append(nn.BatchNorm1d(i)) \n",
    "            layerlist.append(nn.Dropout(p))\n",
    "            input_size = i \n",
    "        layerlist.append(nn.Linear(layer_size[-1],out_size))\n",
    "\n",
    "        self.layers = nn.Sequential(*layerlist) #* is same as nn.ModuleList\n",
    "            \n",
    "    def forward(self, x_cat,x_cont):\n",
    "        #x_cat.shape:  (sample_size, 5)\n",
    "        #x_cont.shape: (sample_size, 2)\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.embed_layer):\n",
    "            embeddings.append(e(x_cat[:,i]))\n",
    "        #embeddings.shape: [(sample_size, emb_size1), \n",
    "        #                   (sample_size, emb_size2), \n",
    "        #                   (sample_size, emb_size3), \n",
    "        #                   (sample_size, emb_size4), \n",
    "        #                   (sample_size, emb_size5)  ]  \n",
    "        x = torch.cat(embeddings,1) \n",
    "        #embeddings.shape: ????\n",
    "        x = self.dropout(x)  #what will happen after dropout? -->\n",
    "        x_cont = self.batchnorm1d(x_cont) #what will happen after batchnorm1d? --->\n",
    "        x = torch.cat([x,x_cont],1) #x: (?????, ???)\n",
    "        x = self.layers(x)\n",
    "        #last x.shape:  ????\n",
    "        return x\n",
    "        \n",
    "#I don't know....this is called designing neural network\n",
    "#you know what is dropout, batchnorm, but combining them is like an ART.....trial and error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_cat = cats[:1]\n",
    "sample_cat.shape  #one sample of five categorical features\n",
    "#['sex', 'education-num', 'marital-status', 'workclass', 'occupation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_cont = conts[:1]\n",
    "sample_cont.shape  #one sample of 2 continous features\n",
    "#['age', 'hours-per-week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then you will understand why         \n",
    "#x_cat.shape:  (sample_size, 5)\n",
    "#x_cont.shape: (sample_size, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(14, 7)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_layer = nn.ModuleList([nn.Embedding(unique, emb_s) for unique, emb_s in emb_size])\n",
    "embed_layer  \n",
    "#what is these five things....what are they\n",
    "#we got five layers\n",
    "#first layer, converts 2 uniques values into vector of size 1\n",
    "test_layer = embed_layer[1]\n",
    "test_layer\n",
    "#what is (14, 7) means?  \n",
    "#it means it turn 14 unique values, each with a vector of size 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number = torch.arange(14)\n",
    "number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Embedding(2, 1)\n",
       "  (1): Embedding(14, 7)\n",
       "  (2): Embedding(6, 3)\n",
       "  (3): Embedding(5, 2)\n",
       "  (4): Embedding(12, 6)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = test_layer(number)\n",
    "output\n",
    "#why we need to create a vector representing each category\n",
    "#--> you can imagine this vector is a numerical representation of each category\n",
    "#why 7?\n",
    "#--> I don't know, but I know 7 is good enough to unique represent category 0 to 13\n",
    "#--> i know good embedding size is half of the unique values....\n",
    "embed_layer\n",
    "#why we need five layers, can I use only 1 layer?\n",
    "#because I have five categorical columns, each with different amount of unique values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.5044]], grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[-0.4602,  1.6739,  0.1997,  0.9124, -0.6707,  0.3829, -0.5612]],\n",
       "        grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[-1.0691, -0.1660,  1.5046]], grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[1.9137, 0.0708]], grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[-0.2984,  1.7227,  0.6665,  1.3135,  0.2768, -1.4090]],\n",
       "        grad_fn=<EmbeddingBackward0>)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = []\n",
    "for i,e in enumerate(embed_layer):\n",
    "    embeddings.append(e(sample_cat[:,i]))\n",
    "    \n",
    "embeddings\n",
    "#embeddings is a randomized values from a uniform distribution (close to 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 6, 3, 2, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#((sample_size, 1), (sample_size, 7), (sample_size, 3), (sample_size, 2), (sample_size, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x1w1 + x2w2 + x3w3 = 10\n",
    "#maybe x = [1, 2, 3]\n",
    "#initially, w is randomized = [0.1, -0.9, 0.8]\n",
    "#but after we backpropagate many times, what is w?\n",
    "#optimized w = [5, 1, 1]\n",
    "\n",
    "#1 * 5 + 2 * 1 + 3 * 1 = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#category = male, female\n",
    "#vector of what size =  5\n",
    "\n",
    "#the feeling of embedding is LIKE THIS\n",
    "\n",
    "#feature1 = length of the hair,   feature2 = weight, feature3 = height, feature4 = kindness, feature5 = violence\n",
    "#  male   =    0.2                    0.5                 0.2                  0.1                  0.8\n",
    "#  female =    0.9                    0.1                 0.2                  0.9                  0.6\n",
    "\n",
    "# why we cannot come up with our own encoding\n",
    "# because neural network can come up with better one, through backpropagation..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n",
      "tensor([[ 0.6704,  0.6410],\n",
      "        [-0.3005,  0.2422],\n",
      "        [ 0.0334, -0.9228],\n",
      "        [ 0.9733,  0.1090],\n",
      "        [ 0.4331,  1.6549]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(99999999)  \n",
    "#in deep learning, usually we cannot do cross-validation\n",
    "#because it's huge dataset...\n",
    "#to get multiple performance metrics, we use different seeds\n",
    "#maybe compare across 5 seeds\n",
    "\n",
    "#create model\n",
    "model = IncomeModel(emb_size, conts.shape[1], len(y.unique()))\n",
    "\n",
    "#try take some sample cat and cont\n",
    "# cat[:1]\n",
    "# conts[:1]\n",
    "\n",
    "#run the model\n",
    "output = model(cats[:5],conts[:5])\n",
    "\n",
    "#check the output \n",
    "print(output.shape)\n",
    "#2 is number of targets in our label\n",
    "#5 is sample size\n",
    "\n",
    "#is the output shape the thing you wan???\n",
    "print(output)\n",
    "#what does this output means\n",
    "#[ 0.6704,  0.6410] is this????\n",
    "#[ not-normalized prob of class 0,  not normalized prob of class 1]\n",
    "# class 0\n",
    "# why they do not sum to 1\n",
    "\n",
    "# my next question, when does my output got normalized?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-4df6fd0e737c>:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  soft_out = softmax(output)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5074, 0.4926],\n",
       "        [0.3676, 0.6324],\n",
       "        [0.7224, 0.2776],\n",
       "        [0.7036, 0.2964],\n",
       "        [0.2276, 0.7724]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax()\n",
    "\n",
    "soft_out = softmax(output)\n",
    "soft_out\n",
    "\n",
    "#hey chaky, but in the last lecture, you NEVER do softmax....never...\n",
    "#so when does softmax happens????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when you say loss = CrossEntropyLoss(yhat, y_test)\n",
    "#CrossEntropyLoss do softmax(yhat) then do cross_entropy with y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0493e-06)\n",
      "Epoch: 1; Loss: 0.25;acc = 0.89\n",
      "500: Loss = 0.2466648519039154;acc = 0.887\n",
      "tensor(-1.0747e-05)\n",
      "Epoch: 51; Loss: 0.25;acc = 0.89\n",
      "500: Loss = 0.2479594349861145;acc = 0.8858333333333334\n",
      "tensor(-1.5936e-06)\n",
      "Epoch: 101; Loss: 0.25;acc = 0.89\n",
      "500: Loss = 0.24557310342788696;acc = 0.887\n",
      "tensor(-3.6470e-06)\n",
      "Epoch: 151; Loss: 0.25;acc = 0.89\n",
      "500: Loss = 0.2455696165561676;acc = 0.8883333333333333\n",
      "tensor(6.5756e-07)\n",
      "Epoch: 201; Loss: 0.24;acc = 0.89\n",
      "500: Loss = 0.24458155035972595;acc = 0.885\n",
      "tensor(-6.7560e-06)\n",
      "Epoch: 251; Loss: 0.24;acc = 0.89\n",
      "500: Loss = 0.239524245262146;acc = 0.886\n",
      "tensor(-5.1672e-06)\n",
      "Epoch: 301; Loss: 0.24;acc = 0.89\n",
      "500: Loss = 0.2395138144493103;acc = 0.8871666666666667\n",
      "tensor(8.0993e-07)\n",
      "Epoch: 351; Loss: 0.24;acc = 0.89\n",
      "500: Loss = 0.23547515273094177;acc = 0.8923333333333333\n",
      "tensor(-2.9191e-06)\n",
      "Epoch: 401; Loss: 0.23;acc = 0.89\n",
      "500: Loss = 0.23362407088279724;acc = 0.8923333333333333\n",
      "tensor(-1.7199e-05)\n",
      "Epoch: 451; Loss: 0.24;acc = 0.89\n",
      "500: Loss = 0.2405625879764557;acc = 0.8866666666666667\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "J_fn      = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "            #how can we know lr = 0.01 is good enough\n",
    "            #why not 0.001, 0.0001\n",
    "            #because our gradients are very small\n",
    "            #it make sense that our lr should NOT be too small!\n",
    "epochs    = 500\n",
    "\n",
    "losses = []\n",
    "accs   = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    yhat = model(cat_train, con_train)\n",
    "    loss = J_fn(yhat, y_train)\n",
    "    \n",
    "    acc  = sklearn.metrics.accuracy_score(torch.max(yhat, 1)[1], y_train)\n",
    "    losses.append(loss)\n",
    "    accs.append(acc)\n",
    "    \n",
    "    #you should NOT print the grad after zero_grad\n",
    "    optimizer.zero_grad()\n",
    "    #you shoudl print the grad after backward\n",
    "    loss.backward()\n",
    "    \n",
    "    # print(model.layers[0].bias.grad)\n",
    "    # print(model.layers[4].weight.grad)\n",
    "    # print(model.layers[4].bias.grad)  \n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 50 == 1:\n",
    "        print(torch.mean(model.layers[0].weight.grad))\n",
    "        print(f\"Epoch: {i}; Loss: {loss:.2f};acc = {acc:.2f}\") \n",
    "        print(f'{epochs}: Loss = {losses[-1]};acc = {acc}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the losses and acc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss = [loss.item() for loss in losses]\n",
    "plt.plot(range(epochs),train_loss)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "#maybe 100 epochs are good enough...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = [acc.item() for acc in accs]\n",
    "plt.plot(range(epochs),train_acc)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "#maybe 100 epochs are enough!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Testing!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('teaching_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "becc4c8e5ad229b2591d820334d85e3db0111492344629bf57f272470dce75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
