{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where are we now\n",
    "1. Python - general problem solving\n",
    "2. Data Science - NumPy, Pandas, Sklearn, Matplotlib \n",
    "3. ML from Scratch - Intuition (so for those who want to further advance....)\n",
    "4. Signal Processing - Energy, Telecommunciations, Biosignals, Time Series\n",
    "5. Deep Learning - PyTorch\n",
    "   1. One of the most popular DL framework (against TensorFlow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning vs. Machine Learning\n",
    "\n",
    "Good News\n",
    "- Deep Learning can automatically feature engineer / feature selection\n",
    "- Deep Learning can benefit from huge amount of data, while Machine Learning cannot\n",
    "  - 100 samples vs 1000 samples, ML will get the same accuracy\n",
    "  - But DL will see increased accuracy\n",
    "- Deep Learning is basically stacking a lot of linear regression together\n",
    "  - DL can learn very complex patterns\n",
    "  - DL is perfect for (1) images, (2) text, (3) signal (very random)\n",
    "\n",
    "Bad News\n",
    "- Deep Learning sucks with small data (vs. Machine Learning) - 5000++ samples\n",
    "- For Tabular Data, Deep Learning will ALMOST ALWAYS LOSE TO gradient boosting (or its variants)\n",
    "  - Gradient Boosting is basically decision trees stacking after one another....\n",
    "  - For most competition, XGBoost and LightGBM are always the winners for tabular data\n",
    "  - If you work in a company, mostly they use tabular data, then you should look for gradient boosting types.... \n",
    "- Deep Learning has NO feature importance; so it's mostly blackbox....(Explanable AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #pip install torch or pip3 install torch or conda install torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.23.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Tensors\n",
    "\n",
    "PyTorch don't use NumPy.  Instead, it has its own data structures, called `Tensor`, which support automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create torch tensors from NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a numpy array of 1 to 5\n",
    "arr = np.arange(1, 6)\n",
    "# arr\n",
    "\n",
    "#print the data type\n",
    "arr.dtype  #int64\n",
    "\n",
    "#print the type()\n",
    "type(arr)  #belongs to Python itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert numpy to tensor\n",
    "\n",
    "#1. from_numpy (copy)\n",
    "torch_arr_from = torch.from_numpy(arr)\n",
    "torch_arr_from.dtype  #torch.int64\n",
    "type(torch_arr_from)  #torch.Tensor\n",
    "torch_arr_from.type() #torch.LongTensor (int64); if torch.IntTensor (int32)\n",
    "                      #torch.FloatTensor (float32); if torch.DoubleTensor (float64)\n",
    "#from_numpy is a copy!!!  This is intended, for easy use between numpy and tensor...\n",
    "# arr[2] = 999\n",
    "# torch_arr_from\n",
    "\n",
    "#2. tensor (not a copy)\n",
    "torch_arr_tensor = torch.tensor(arr)  #everything is the same, except it's NOT a copy\n",
    "arr[2] = 9999999\n",
    "torch_arr_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('teaching_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "becc4c8e5ad229b2591d820334d85e3db0111492344629bf57f272470dce75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
