{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression from Scratch\n",
    "\n",
    "- Fundamental of all deep learning operations, concerning classification\n",
    "- Multinomial - used multinomial distributions to derive the likelihood.....\n",
    "              - multiclass version of logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8103.083927575384"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(9)\n",
    "\n",
    "#e^9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Four steps:\n",
    "\n",
    "1.  Predict - `X.shape = (m, n); y.shape = (m, k); theta/w/weights.shape = (n, k)`\n",
    "    \n",
    "    What is $g(x)$ input and output?\n",
    "\n",
    "    input  = X:    (m, n)\n",
    "    output = yhat: (m, k) \n",
    "\n",
    "    Mathematially, $g(x)$ is an adapted version of the sigmoid formula, so that it outputs (m, k).  We called this $g(x)$ the **softmax** function.\n",
    "\n",
    "    $$ h = P(y = c | \\theta) = \\frac{e^{\\mathbf{X}\\theta_c}}{\\sum_{i=1}^k e^{\\mathbf{X}\\theta_i}}$$\n",
    "\n",
    "    Division makes sure the probability sums to 1.\n",
    "\n",
    "    Why $e$?\n",
    "    - $e$ always give positive number, so it's a perfect guy to calculate probability!\n",
    "    - But hey, chaky, $e$ to the power of something gives me some number, like very high number, like 8103...., so how it can be probability? \n",
    "      - Ans: very simple, just divide by all e, then they will be frac of 1\n",
    "    - Derivative of e cancels nicely with log\n",
    "    \n",
    "    Why is called softmax?\n",
    "    - what is hardmax ==> [2, 3, 4] ==> [0, 0, 1] \n",
    "      - The y vector has only probability of either 1 or 0\n",
    "    - softmax: ==> [2, 3, 4] ==> [0.05, 0.15, 0.8]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "2.  Find the gradient\n",
    "   \n",
    "    2.1 We must first define the $J(\\theta)$\n",
    "        \n",
    "        Note: \n",
    "        - the behavior of J is that when y, yhat= (1,1) | (0, 0) , then J = 0\n",
    "        - the behavior of J is that when y, yhat= (0,1) | (1, 0) , then J = very high\n",
    "\n",
    "\n",
    "    $$J(\\theta) = \\sum_{i=0}^m -( \\mathbf{y}^{(i)} * \\log{\\mathbf{h}}^{(i)} + (1 - \\mathbf{y}^{(i)})\\log(1 - {\\mathbf{h}}^{(i)}))$$\n",
    "             \n",
    "\n",
    "    2.2 $$\\frac{\\partial J}{\\partial \\theta_j} = \\mathbf{X}^\\top (\\hat{\\mathbf{y}} - \\mathbf{y})$$\n",
    "\n",
    "3. Update the theta \n",
    "\n",
    "    $$\\theta = \\theta - \\alpha * \\frac{\\partial J}{\\partial \\theta_j}$$\n",
    "\n",
    "4. Repeat 1, 2, 3 until either (1) `num_epochs` reach limit, or (2) early stopping (which I asked you to implement in your assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[1, 3, 8], [4, 8, 1]]) #X with two samples, and three features\n",
    "X.shape  #(m, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initial y = [2, 3]  #k = 3, which i have 3 classes, 1, 2, 3\n",
    "#BUT in ML/DL, we DON'T DEFINE y LIKE THIS\n",
    "#We define as a one-hot encoded vectors\n",
    "\n",
    "#for the first sample, the probability of being class 1 = 0; class 2 = 1; class 3 = 0\n",
    "y = np.array([ [0, 1, 0] , [0, 0, 1]  ])\n",
    "y.shape #(m, k) where k is number of classes\n",
    "\n",
    "#with this format, you can get probability of each class, not absolutely the class \n",
    "#which is silly....because every prediction is probabilistic....\n",
    "\n",
    "#i also want to let you know how yhat typically looks like\n",
    "#the behavior is the the probability sums to 1\n",
    "#the probability of class 1 is 0.05, class 2 is 0.86, class 3 is 0.09\n",
    "yhat_example = np.array([  [0.05, 0.86, 0.09 ], [0.3, 0.3, 0.4]   ])\n",
    "yhat_example.shape #(m, k)\n",
    "\n",
    "#during like real prediction, we take the index with the highest probability\n",
    "#this is what i want - [2, 3]\n",
    "#so how to convert yhat_example to [2, 3]; Ans: use argmax\n",
    "np.argmax(yhat_example, axis=1) #since the index starts at 0\n",
    "#so here, let's assume the class is 0, 1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "X: (m, n)\n",
    "w/theta/weight: (?, ?)\n",
    "y: (m, k)\n",
    "\n",
    "(m, n) @ (n, k) = (m, k)\n",
    "'''\n",
    "# w = np.ones((X.shape[1], len(y.unique())))\n",
    "#assuming my y is not yet one-hot encoded...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ h = P(y = c | \\theta) = \\frac{e^{\\mathbf{X}\\theta_c}}{\\sum_{i=1}^k e^{\\mathbf{X}\\theta_i}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3, 8],\n",
       "       [4, 8, 1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5],\n",
       "       [6, 7, 8]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = np.arange(9).reshape((X.shape[1], 3))\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so please write the softmax function,\n",
    "#can calculate the yhat with the given theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('teaching_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "becc4c8e5ad229b2591d820334d85e3db0111492344629bf57f272470dce75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
