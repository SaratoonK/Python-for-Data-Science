{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression from Scratch\n",
    "\n",
    "- Fundamental of all deep learning operations, concerning classification\n",
    "- Multinomial - used multinomial distributions to derive the likelihood.....\n",
    "              - multiclass version of logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8103.083927575384"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(9)\n",
    "\n",
    "#e^9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Four steps:\n",
    "\n",
    "1.  Predict - `X.shape = (m, n); y.shape = (m, k); theta/w/weights.shape = (n, k)`\n",
    "    \n",
    "    What is $g(x)$ input and output?\n",
    "\n",
    "    input  = X:    (m, n)\n",
    "    output = yhat: (m, k) \n",
    "\n",
    "    Mathematially, $g(x)$ is an adapted version of the sigmoid formula, so that it outputs (m, k).  We called this $g(x)$ the **softmax** function.\n",
    "\n",
    "    $$ h = P(y = c | \\theta) = \\frac{e^{\\mathbf{X}\\theta_c}}{\\sum_{i=1}^k e^{\\mathbf{X}\\theta_i}}$$\n",
    "\n",
    "    Division makes sure the probability sums to 1.\n",
    "\n",
    "    Why $e$?\n",
    "    - $e$ always give positive number, so it's a perfect guy to calculate probability!\n",
    "    - But hey, chaky, $e$ to the power of something gives me some number, like very high number, like 8103...., so how it can be probability? \n",
    "      - Ans: very simple, just divide by all e, then they will be frac of 1\n",
    "    - Derivative of e cancels nicely with log\n",
    "    \n",
    "    Why is called softmax?\n",
    "    - what is hardmax ==> [2, 3, 4] ==> [0, 0, 1] \n",
    "      - The y vector has only probability of either 1 or 0\n",
    "    - softmax: ==> [2, 3, 4] ==> [0.05, 0.15, 0.8]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "2.  Find the gradient\n",
    "   \n",
    "    2.1 We must first define the $J(\\theta)$\n",
    "        \n",
    "        Note: \n",
    "        - if y = [0, 0, 1, 0], and yhat = [0.1, 0.1, 0.7, 0.1], then J must be low\n",
    "          - y * h = [0, 0, 0.7, 0]\n",
    "        - if y = [0, 0, 1, 0], and yhat = [0.9, 0.03, 0.03, 0.04], then J must be high\n",
    "          - y * h = [0, 0, 0.03, 0]\n",
    "\n",
    "    $$J = -\\sum_{i=1}^m \\mathbf{Y}^{(i)} * \\log{\\mathbf{H}}^{(i)}$$\n",
    "             \n",
    "\n",
    "    2.2 $$\\frac{\\partial J}{\\partial \\theta_j} = \\mathbf{X}^\\top (\\mathbf{H} - \\mathbf{Y})$$\n",
    "\n",
    "3. Update the theta \n",
    "\n",
    "    $$\\theta = \\theta - \\alpha * \\frac{\\partial J}{\\partial \\theta_j}$$\n",
    "\n",
    "4. Repeat 1, 2, 3 until either (1) `num_epochs` reach limit, or (2) early stopping (which I asked you to implement in your assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[1, 3, 8], [4, 8, 1]]) #X with two samples, and three features\n",
    "X.shape  #(m, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initial y = [2, 3]  #k = 3, which i have 3 classes, 1, 2, 3\n",
    "#BUT in ML/DL, we DON'T DEFINE y LIKE THIS\n",
    "#We define as a one-hot encoded vectors\n",
    "\n",
    "#for the first sample, the probability of being class 1 = 0; class 2 = 1; class 3 = 0\n",
    "y = np.array([ [0, 1, 0] , [0, 0, 1]  ])\n",
    "y.shape #(m, k) where k is number of classes\n",
    "\n",
    "#with this format, you can get probability of each class, not absolutely the class \n",
    "#which is silly....because every prediction is probabilistic....\n",
    "\n",
    "#i also want to let you know how yhat typically looks like\n",
    "#the behavior is the the probability sums to 1\n",
    "#the probability of class 1 is 0.05, class 2 is 0.86, class 3 is 0.09\n",
    "yhat_example = np.array([  [0.05, 0.86, 0.09 ], [0.3, 0.3, 0.4]   ])\n",
    "yhat_example.shape #(m, k)\n",
    "\n",
    "#during like real prediction, we take the index with the highest probability\n",
    "#this is what i want - [2, 3]\n",
    "#so how to convert yhat_example to [2, 3]; Ans: use argmax\n",
    "np.argmax(yhat_example, axis=1) #since the index starts at 0\n",
    "#so here, let's assume the class is 0, 1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "X: (m, n)\n",
    "w/theta/weight: (?, ?)\n",
    "y: (m, k)\n",
    "\n",
    "(m, n) @ (n, k) = (m, k)\n",
    "'''\n",
    "# w = np.ones((X.shape[1], len(y.unique())))\n",
    "#assuming my y is not yet one-hot encoded...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ h = P(y = c | \\theta) = \\frac{e^{\\mathbf{X}\\theta_c}}{\\sum_{i=1}^k e^{\\mathbf{X}\\theta_i}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3, 8],\n",
       "       [4, 8, 1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5],\n",
       "       [6, 7, 8]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = np.arange(9).reshape((X.shape[1], 3))\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False]\n"
     ]
    }
   ],
   "source": [
    "#so please write the softmax function,\n",
    "#can calculate the yhat with the given theta\n",
    "def softmax(input):\n",
    "    return np.exp(input) / np.sum(np.exp(input), axis=1, keepdims=True)\n",
    "\n",
    "yhat = softmax(X @ theta)\n",
    "sum  = yhat.sum(axis = 1)\n",
    "\n",
    "assert np.all(sum == 1)\n",
    "assert yhat.shape == (X.shape[0], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J = -\\sum_{i=1}^m \\mathbf{y}^{(i)} * \\log{\\mathbf{h}}^{(i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good:  0.10536051565782628\n",
      "Bad:  2.3025850929940455\n"
     ]
    }
   ],
   "source": [
    "#1. Implement this loss function - cross entropy\n",
    "def ce(Y, H):\n",
    "    return -np.sum(Y * np.log(H)) #because this should give me one number, because J is a number\n",
    "\n",
    "#2. Create a y, a good yhat, a bad yhat\n",
    "Y      = np.array([ [0, 0, 1, 0] ])  #(1, k)\n",
    "good_Y = np.array([ [0.04, 0.03, 0.9, 0.03] ]) \n",
    "bad_Y  = np.array([ [0.3,  0.3,  0.1, 0.3 ] ]) \n",
    "\n",
    "#3. And check the loss.  The good yhat should have low loss, and bad yhat should have high loss\n",
    "print(\"Good: \", ce(Y, good_Y))\n",
    "print(\"Bad: \",  ce(Y, bad_Y))\n",
    "\n",
    "#4. Optional: try to understand what does `-log` do to h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "h = np.linspace(0.01, 0.99, 1000)\n",
    "plt.plot(h, -np.log(h))\n",
    "plt.xlabel(\"prob\")\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "#three purposes of log\n",
    "\n",
    "#1. by doing this, you will notice one thing:\n",
    "#small probability will be kept small\n",
    "#only the very big probabiliy will be enlarged\n",
    "#Recall the definition of softmax\n",
    "\n",
    "#2. it keeps small probability in check\n",
    "    #i.e., no errors like 0.00000000000001 == 0\n",
    "    #by doing np.log, this value is kept\n",
    "\n",
    "#3. log deals very nicely with e, they cancel one another\n",
    "    #the magic happens when we find the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.10536051565782628"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's code multinomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "#1. load sklearn iris dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "#1.1 optionally, act like you do some EDA with plotting\n",
    "\n",
    "#2. perform split, standardization, whatever you call\n",
    "\n",
    "#hint: check how your y looks like\n",
    "#2.1 convert your y to one-hot encoded y\n",
    "\n",
    "#3. implement class MultinomialLogisticRegression with fit() and predict()\n",
    "\n",
    "#4. test with test set, and print sklearn.metrics classification reports, as well sklearn.metrics confusion matrix\n",
    "    #use argmax, because your test set is NOT one-hot encoded....\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('teaching_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "becc4c8e5ad229b2591d820334d85e3db0111492344629bf57f272470dce75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
