{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayesian - Gaussian\n",
    "\n",
    "- Logistic Regression and Naive Baye are two baseline algorithms for classification\n",
    "- Baselines mean they serve like the mininum\n",
    "- When you do classification, it's a no brainer to use Logistic and Naive\n",
    "  - they are so simple yet so effective\n",
    "- **Key idea for GNB**: use the normal distribution to estimate y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(y|x) = \\frac{P(x|y)P(y)}{P(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous class, you estimate $P(y | x)$ directly using gradient descent.\n",
    "\n",
    "But we can do another way, that is, using naive baye algorithm, i.e., if we can find $P(x | y)$, then we can find $P(y | x)$ ===> \"generative algorithms\"\n",
    "\n",
    "So we can ask something like this right, e.g., $P(y | x = 4)$ ==>\n",
    "\n",
    "$$P(y = class 1 | x = 4)$$ \n",
    "$$P(y = class 2 | x = 4)$$ \n",
    "and so on...\n",
    "\n",
    "$$\\frac{P(x|y = class 1)P(y = class 1)}{P(x)}$$ \n",
    "$$\\frac{P(x|y = class 2)P(y = class 2)}{P(x)}$$ \n",
    "and so on...\n",
    "\n",
    "We can ignore $P(x)$, and compare only the numerator (top guy)\n",
    "\n",
    "$$P(x|y = class 1)P(y = class 1)$$ \n",
    "$$P(x|y = class 2)P(y = class 2)$$ \n",
    "\n",
    "So we have two components here, the easy one is $P(y = class 1)$ and $P(y = class 2)$, they are basically:\n",
    "\n",
    "$$P(y = 1) = \\frac{\\sum_{i=1}^m 1(y=1)}{m}$$\n",
    "$$P(y = 0) = \\frac{\\sum_{i=1}^m 1(y=0)}{m}$$\n",
    "\n",
    "Now the question is the left part, which is $P(x|y = class 1)$, how to find?\n",
    "\n",
    "Answer: not easy!\n",
    "\n",
    "Naive answer: assume $P(x | y)$ follows some statistical distribution.\n",
    "\n",
    "- if your x is continuous, you can assume gaussian distribution\n",
    "- if your x is discrete, you can assume multinomial distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6, 0.4)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.array([\n",
    "    [1, 2, 3], \n",
    "    [4, 5, 6], \n",
    "    [7, 8, 9],\n",
    "    [10, 11, 12],\n",
    "    [11, 12, 13]\n",
    "])\n",
    "\n",
    "y_train = np.array([0, 0, 0, 1, 1])\n",
    "\n",
    "cond0 = y_train == 0\n",
    "cond1 = y_train == 1\n",
    "\n",
    "m0 = len(y_train[cond0])\n",
    "m1 = len(y_train[cond1])\n",
    "m  = len(y_train)\n",
    "\n",
    "#simple exercise: find P(y = 0), P(y = 1) ==> priors\n",
    "prior0 = m0 / m\n",
    "prior1 = m1 / m\n",
    "\n",
    "prior0, prior1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to find P(x | y = 0), P(x | y = 1)\n",
    "#to make it, we need the normal distribution,\n",
    "#before we have the normal distribution, we need to find the mean and std\n",
    "\n",
    "#mean of each class of each feature\n",
    "#std  of each class of each feature\n",
    "\n",
    "#mean.shape: (k, n) or (n, k) #here k = #class, n = #features\n",
    "#std.shape:  (k, n)\n",
    "\n",
    "#please find these mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  [[ 4.   5.   6. ]\n",
      " [10.5 11.5 12.5]]\n",
      "Std:  [[2.44948974 2.44948974 2.44948974]\n",
      " [0.5        0.5        0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "def mean_std(X, y, k):\n",
    "    #create empy mean and std\n",
    "    n    = X.shape[1]\n",
    "    mean = np.zeros((k, n))\n",
    "    std  = np.zeros((k, n))\n",
    "    \n",
    "    #for loop, loop each cond\n",
    "    for label in range(k):\n",
    "        mean[label, :] = X[y == label].mean(axis=0)\n",
    "        std[label, :]  = X[y == label].std(axis=0)\n",
    "    return mean, std\n",
    "\n",
    "mean, std = mean_std(X_train, y_train, 2)\n",
    "\n",
    "print(\"Mean: \", mean)\n",
    "print(\"Std: \", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can now create the normal distribution - pdf - probability density function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(x \\mid y=1 ; \\mu_1, \\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e ^{-\\frac{(x-\\mu_1)^{2}}{2\\sigma^{2}}}$$\n",
    "$$ P(x \\mid y=0 ; \\mu_0, \\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e ^{-\\frac{(x-\\mu_0)^{2}}{2\\sigma^{2}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. create a function called gaussian_pdf(X_test, mean, std)\n",
    "    #return the probability\n",
    "    \n",
    "    \n",
    "    \n",
    "#2. Create some X_test, and try to predict the y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proof is as follows:\n",
    "\n",
    "- the probabilty of two events x and y happening, $P(x \\cap y)$ is the probability of $x$ or $P(x)$, times the probability of $y$ given that $P(x)$ has occured, $P(y \\mid x)$\n",
    "\n",
    "$$ P(x \\cap y) = P(x)P(y \\mid x)$$\n",
    "\n",
    "- on the other hand, the probability of $x$ and $y$ is also equal to the probability of $y$ timese the probabilty of $x$ given $y$\n",
    "\n",
    "$$ P(x \\cap y) = P(y)P(x \\mid y)$$\n",
    "\n",
    "- Equating the two yields:\n",
    "\n",
    "$$ P(x)P(y \\mid x) = P(y)P(x \\mid y)$$\n",
    "\n",
    "- Thus\n",
    "\n",
    "$$ P(y \\mid x) = \\frac{P(y)P(x \\mid y)}{P(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(x \\mid y=1 ; \\mu_1, \\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e ^{-\\frac{(x-\\mu_1)^{2}}{2\\sigma^{2}}}$$\n",
    "$$ P(x \\mid y=0 ; \\mu_0, \\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e ^{-\\frac{(x-\\mu_0)^{2}}{2\\sigma^{2}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('teaching_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "becc4c8e5ad229b2591d820334d85e3db0111492344629bf57f272470dce75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
