{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayesian - Gaussian\n",
    "\n",
    "- Logistic Regression and Naive Baye are two baseline algorithms for classification\n",
    "- Baselines mean they serve like the mininum\n",
    "- When you do classification, it's a no brainer to use Logistic and Naive\n",
    "  - they are so simple yet so effective\n",
    "- **Key idea for GNB**: use the normal distribution to estimate y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(y|x) = \\frac{P(x|y)P(y)}{P(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous class, you estimate $P(y | x)$ directly using gradient descent.\n",
    "\n",
    "But we can do another way, that is, using naive baye algorithm, i.e., if we can find $P(x | y)$, then we can find $P(y | x)$ ===> \"generative algorithms\"\n",
    "\n",
    "So we can ask something like this right, e.g., $P(y | x = 4)$ ==>\n",
    "\n",
    "$$P(y = class 1 | x = 4)$$ \n",
    "$$P(y = class 2 | x = 4)$$ \n",
    "and so on...\n",
    "\n",
    "$$\\frac{P(x|y = class 1)P(y = class 1)}{P(x)}$$ \n",
    "$$\\frac{P(x|y = class 2)P(y = class 2)}{P(x)}$$ \n",
    "and so on...\n",
    "\n",
    "We can ignore $P(x)$, and compare only the numerator (top guy)\n",
    "\n",
    "$$P(x|y = class 1)P(y = class 1)$$ \n",
    "$$P(x|y = class 2)P(y = class 2)$$ \n",
    "\n",
    "So we have two components here, the easy one is $P(y = class 1)$ and $P(y = class 2)$, they are basically:\n",
    "\n",
    "$$P(y = 1) = \\frac{\\sum_{i=1}^m 1(y=1)}{m}$$\n",
    "$$P(y = 0) = \\frac{\\sum_{i=1}^m 1(y=0)}{m}$$\n",
    "\n",
    "Now the question is the left part, which is $P(x|y = class 1)$, how to find?\n",
    "\n",
    "Answer: not easy!\n",
    "\n",
    "Naive answer: assume $P(x | y)$ follows some statistical distribution.\n",
    "\n",
    "- if your x is continuous, you can assume gaussian distribution\n",
    "- if your x is discrete, you can assume multinomial distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6, 0.4)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.array([\n",
    "    [1, 2, 3], \n",
    "    [4, 5, 6], \n",
    "    [7, 8, 9],\n",
    "    [10, 11, 12],\n",
    "    [11, 12, 13]\n",
    "])\n",
    "\n",
    "y_train = np.array([0, 0, 0, 1, 1])\n",
    "\n",
    "cond0 = y_train == 0\n",
    "cond1 = y_train == 1\n",
    "\n",
    "m0 = len(y_train[cond0])\n",
    "m1 = len(y_train[cond1])\n",
    "m  = len(y_train)\n",
    "\n",
    "#simple exercise: find P(y = 0), P(y = 1) ==> priors\n",
    "prior0 = m0 / m\n",
    "prior1 = m1 / m\n",
    "\n",
    "prior0, prior1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to find P(x | y = 0), P(x | y = 1)\n",
    "#to make it, we need the normal distribution,\n",
    "#before we have the normal distribution, we need to find the mean and std\n",
    "\n",
    "#mean of each class of each feature\n",
    "#std  of each class of each feature\n",
    "\n",
    "#mean.shape: (k, n) or (n, k) #here k = #class, n = #features\n",
    "#std.shape:  (k, n)\n",
    "\n",
    "#please find these mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  [[ 4.   5.   6. ]\n",
      " [10.5 11.5 12.5]]\n",
      "Std:  [[2.44948974 2.44948974 2.44948974]\n",
      " [0.5        0.5        0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "def mean_std(X, y, k):\n",
    "    #create empy mean and std\n",
    "    n    = X.shape[1]\n",
    "    mean = np.zeros((k, n))\n",
    "    std  = np.zeros((k, n))\n",
    "    \n",
    "    #for loop, loop each cond\n",
    "    for label in range(k):\n",
    "        mean[label, :] = X[y == label].mean(axis=0)\n",
    "        std[label, :]  = X[y == label].std(axis=0)\n",
    "    return mean, std\n",
    "\n",
    "mean, std = mean_std(X_train, y_train, 2)\n",
    "\n",
    "print(\"Mean: \", mean)\n",
    "print(\"Std: \", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can now create the normal distribution - pdf - probability density function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(x \\mid y=1 ; \\mu_1, \\sigma_1^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma_1^{2}}}e ^{-\\frac{(x-\\mu_1)^{2}}{2\\sigma_1^{2}}}$$\n",
    "$$ P(x \\mid y=0 ; \\mu_0, \\sigma_0^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma_0^{2}}}e ^{-\\frac{(x-\\mu_0)^{2}}{2\\sigma_0^{2}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.83941449e-01 4.83941449e-01 4.83941449e-01]\n",
      " [3.24927207e-79 3.24927207e-79 1.10614191e-49]]\n"
     ]
    }
   ],
   "source": [
    "#1. create a function called gaussian_pdf(X_test, mean, std)\n",
    "    #return the probability\n",
    "    \n",
    "#Note that this mean and std COMES FROM training set....\n",
    "#but when we predict, we use X_test\n",
    "def gaussian_pdf(X_test, mean, std):\n",
    "    left = 1 / (np.sqrt(2 * np.pi) * std)\n",
    "    power = (X_test - mean) ** 2 / (2 * (std ** 2))\n",
    "    right = np.exp(-power)\n",
    "    return left * right\n",
    "    \n",
    "#2. Create some X_test, and try to predict the y\n",
    "X_test = np.array([    [11, 12, 13], [1, 2, 5]   ])\n",
    "\n",
    "pdf0 = gaussian_pdf(X_test, mean[0, :], std[0, :])\n",
    "pdf1 = gaussian_pdf(X_test, mean[1, :], std[1, :])\n",
    "\n",
    "print(pdf1) \n",
    "#m, n\n",
    "#0.0081087 is the probability of sample1 of feature1 for class0\n",
    "#0.07693316 is the probability of sample2 of feature1 for class0\n",
    "#interpret: sample2 has a much higher probability to become class0\n",
    "#interpret: sample1 has a much higher probabilty to become class1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#this is the naive part, which assumes that each features are independent\n",
    "#thus the final probability is simply multiplication of it (principle of IID)\n",
    "$$P(x \\mid y) = \\prod_{i=1}^n P( x_i \\mid y )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.06726389e-08 8.86891295e-04] [1.13338761e-001 1.16783908e-206]\n"
     ]
    }
   ],
   "source": [
    "total_likelihood0 = np.prod(pdf0, axis=1)\n",
    "total_likelihood1 = np.prod(pdf1, axis=1)\n",
    "\n",
    "print(total_likelihood0, total_likelihood1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(x|y = class 1)P(y = class 1)$$ \n",
    "$$P(x|y = class 2)P(y = class 2)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.24035833e-08 5.32134777e-04] [4.53355045e-002 4.67135631e-207]\n"
     ]
    }
   ],
   "source": [
    "posterior0 = prior0 * total_likelihood0\n",
    "posterior1 = prior1 * total_likelihood1\n",
    "\n",
    "print(posterior0, posterior1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = 1 * (posterior1 > posterior0)\n",
    "\n",
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('teaching_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "becc4c8e5ad229b2591d820334d85e3db0111492344629bf57f272470dce75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
